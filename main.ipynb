{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fbd0df4ee4cac91",
   "metadata": {},
   "source": [
    "# Создание модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df300eec7b8ef8a",
   "metadata": {},
   "source": [
    "Модель сначала сжимает входные значения через Linear, затем LSTM извлекает последовательные зависимости. <br>\n",
    "Attention позволяет выбрать наиболее значимые участки этой памяти. Маска позволяет игнорировать паддинги. <br>\n",
    "В результате модель агрегирует информацию в контекстный вектор и делает регрессионное предсказание следующего значения <br>"
   ]
  },
  {
   "cell_type": "code",
   "id": "619372fe7f7abc2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T16:25:43.084959Z",
     "start_time": "2025-05-18T16:25:43.081332Z"
    }
   },
   "source": [
    "from models.lstm_with_attention import LSTMWithAttention"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "81fb85829c64f328",
   "metadata": {},
   "source": [
    "### Визуализация модели"
   ]
  },
  {
   "cell_type": "code",
   "id": "985b9c9a7a678d81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T16:28:50.142638Z",
     "start_time": "2025-05-18T16:28:50.040811Z"
    }
   },
   "source": [
    "import torch\n",
    "from torchview import draw_graph\n",
    "\n",
    "# Параметры модели\n",
    "input_dim = 1\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "seq_len = 20\n",
    "batch_size = 2\n",
    "\n",
    "x = torch.randn(batch_size, seq_len)\n",
    "mask = torch.ones(batch_size, seq_len).bool()\n",
    "\n",
    "# Инициализация модели\n",
    "model = LSTMWithAttention(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, dropout=0.1, bidirectional=False)\n",
    "\n",
    "print(f\"Total parameters:     {model.count_parameters():,}\")\n",
    "print(f\"Trainable parameters: {model.count_parameters():,}\")\n",
    "\n",
    "\n",
    "draw_graph(model, input_data=(x, mask), graph_name='LSTM with Attention', expand_nested=True, roll=True).visual_graph.render(format='svg')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "(process:42456): Pango-WARNING **: 00:28:50.126: couldn't load font \"Linux libertine Not-Rotated 10\", falling back to \"Sans Not-Rotated 10\", expect ugly output.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:     264,962\n",
      "Trainable parameters: 264,962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LSTM with Attention.gv.svg'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "b4cf897d1e152ebc",
   "metadata": {},
   "source": [
    "# Генерация данных\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3c7bdedbebc25dd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T16:15:58.945944Z",
     "start_time": "2025-05-18T16:15:41.151671Z"
    }
   },
   "source": [
    "from data.generators import generate_data\n",
    "\n",
    "data, labels, masks = generate_data(num_samples=1_000_000)\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "print(\"Masks shape:\", masks.shape)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sequences:  24%|██▍       | 237690/1000000 [00:15<00:51, 14867.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mdata\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mgenerators\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m generate_data\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m data, labels, masks = \u001B[43mgenerate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1_000_000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mData shape:\u001B[39m\u001B[33m\"\u001B[39m, data.shape)\n\u001B[32m      5\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mLabels shape:\u001B[39m\u001B[33m\"\u001B[39m, labels.shape)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mH:\\Projects\\Python\\Data Science\\numerical-series-predictor\\data\\generators.py:199\u001B[39m, in \u001B[36mgenerate_data\u001B[39m\u001B[34m(num_samples, max_length, seed)\u001B[39m\n\u001B[32m    196\u001B[39m y = sequence[-\u001B[32m1\u001B[39m]   \u001B[38;5;66;03m# target next value\u001B[39;00m\n\u001B[32m    197\u001B[39m padding = max_length - seq_len\n\u001B[32m--> \u001B[39m\u001B[32m199\u001B[39m padded_x = \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpad\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    200\u001B[39m mask = torch.tensor([\u001B[32m1\u001B[39m] * seq_len + [\u001B[32m0\u001B[39m] * padding, dtype=torch.bool)\n\u001B[32m    202\u001B[39m data.append(padded_x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\functional.py:5209\u001B[39m, in \u001B[36mpad\u001B[39m\u001B[34m(input, pad, mode, value)\u001B[39m\n\u001B[32m   5202\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m mode == \u001B[33m\"\u001B[39m\u001B[33mreplicate\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m   5203\u001B[39m             \u001B[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put.\u001B[39;00m\n\u001B[32m   5204\u001B[39m             \u001B[38;5;66;03m# importlib is required because the import cannot be top level\u001B[39;00m\n\u001B[32m   5205\u001B[39m             \u001B[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001B[39;00m\n\u001B[32m   5206\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m importlib.import_module(\n\u001B[32m   5207\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtorch._decomp.decompositions\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   5208\u001B[39m             )._replication_pad(\u001B[38;5;28minput\u001B[39m, pad)\n\u001B[32m-> \u001B[39m\u001B[32m5209\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_C\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_nn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpad\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca47be252716557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import build_dataloaders\n",
    "\n",
    "train_loader, val_loader, test_loader = build_dataloaders(data, labels, masks, split=(0.8, 0.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a592a64cd3e1c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af032f9c45afa877",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde5b93441d0dcf",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch CUDA status: \n",
      "\t✅ Available \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 7957136.0464 | Val Loss: 7623519.1869\n",
      "Example input: [7.8457403 2.8538585 2.0083718 1.5180016 1.6150894 1.5737698 1.6822139\n",
      " 1.3469346 1.8272529 1.4052224 1.3979295 1.4048356 1.0697714 1.3430461\n",
      " 1.3307256 0.9500988]\n",
      "Example target: 1.2512695789337158\n",
      "Example prediction: -0.14015483856201172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 7187625.3435 | Val Loss: 6904709.3477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 6509382.4695 | Val Loss: 6223053.2108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 5917429.5591 | Val Loss: 5665147.0888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 5423526.9223 | Val Loss: 5232672.6995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train Loss: 4996381.6710 | Val Loss: 4726086.2742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train Loss: 4529546.0835 | Val Loss: 4272316.4435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train Loss: 4247364.0840 | Val Loss: 3896796.4607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Train Loss: 3865682.9015 | Val Loss: 3584806.0433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Train Loss: 3579224.2274 | Val Loss: 3279558.8927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Train Loss: 3280051.6591 | Val Loss: 2980968.2701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Train Loss: 3008078.1782 | Val Loss: 2720398.8204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Train Loss: 2835497.0900 | Val Loss: 2781793.2676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Train Loss: 2565468.1585 | Val Loss: 2226185.6577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Train Loss: 2453172.3503 | Val Loss: 2107748.4486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16] Train Loss: 2358667.7150 | Val Loss: 1922177.4947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17] Train Loss: 2088518.9918 | Val Loss: 1792082.8666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18] Train Loss: 2096576.2416 | Val Loss: 1824506.6147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19] Train Loss: 1977042.7256 | Val Loss: 1545631.0965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20] Train Loss: 1791105.2993 | Val Loss: 1404366.5393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21] Train Loss: 1688854.4196 | Val Loss: 1314974.8765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22] Train Loss: 1585496.1097 | Val Loss: 1211513.6850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23] Train Loss: 1521311.1871 | Val Loss: 1175109.4426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24] Train Loss: 1463469.3427 | Val Loss: 1024020.8407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25] Train Loss: 1366838.8935 | Val Loss: 1261856.6523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26] Train Loss: 1363415.6447 | Val Loss: 1081075.2365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 27] Train Loss: 1302634.8457 | Val Loss: 923608.1406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 28] Train Loss: 1218429.1651 | Val Loss: 835073.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 29] Train Loss: 1144705.4867 | Val Loss: 795965.4185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 30] Train Loss: 1089783.5336 | Val Loss: 683721.5256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 31] Train Loss: 1038287.7025 | Val Loss: 702043.0234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 32] Train Loss: 990549.7339 | Val Loss: 662496.3730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 33] Train Loss: 946261.3119 | Val Loss: 695280.4111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 34] Train Loss: 915177.3741 | Val Loss: 651878.7291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 35] Train Loss: 878862.6178 | Val Loss: 601688.8259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 36] Training:  32%|███▏      | 3962/12500 [00:32<01:02, 136.04it/s, loss=3.34e+5]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from training.trainer import train_model\n",
    "\n",
    "print('Torch CUDA status: \\n\\t%s' %\n",
    "      '✅ Available' if torch.cuda.is_available() else '❌ NOT available', '\\n')\n",
    "\n",
    "# Обучение\n",
    "trained_model, train_losses, val_losses = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=2_000,\n",
    "    patience=20,\n",
    "    lr=1e-3,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    checkpoint_every=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dca247dbb9df561",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213bd20aee575b89",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trained_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# TODO: delete later\u001B[39;00m\n\u001B[32m      2\u001B[39m torch.save({\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mconfig\u001B[39m\u001B[33m\"\u001B[39m: \u001B[43mtrained_model\u001B[49m.get_config(),\n\u001B[32m      4\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mstate_dict\u001B[39m\u001B[33m\"\u001B[39m: trained_model.state_dict(),\n\u001B[32m      5\u001B[39m }, \u001B[33m\"\u001B[39m\u001B[33mweights/HOPE_IT_WORKS_v2.pt\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      6\u001B[39m torch.save(trained_model, \u001B[33m\"\u001B[39m\u001B[33msaved_models/lstm_full_v2.pt\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      7\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mModel saved ✅\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'trained_model' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: delete later\n",
    "torch.save({\n",
    "    \"config\": trained_model.get_config(),\n",
    "    \"state_dict\": trained_model.state_dict(),\n",
    "}, \"weights/weird ones/HOPE_IT_WORKS_v2.pt\")\n",
    "torch.save(trained_model, \"saved_models/lstm_full_v2.pt\")\n",
    "print(\"Model saved ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e64643bc7b8066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     \"input_dim\": model.input_linear.in_features,\n",
    "#     \"hidden_dim\": model.lstm.hidden_size,\n",
    "#     \"num_layers\": model.lstm.num_layers,\n",
    "#     \"dropout\": model.lstm.dropout\n",
    "# }\n",
    "\n",
    "torch.save({\n",
    "    # \"config\": config,\n",
    "    \"config\": trained_model.get_config(),\n",
    "    \"state_dict\": trained_model.state_dict(),\n",
    "}, \"weights/lstm_epoch234_val123.pt\")\n",
    "torch.save(trained_model, \"saved_models/lstm_full_v2.pt\")\n",
    "print(\"Model saved ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99887654843747d",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0845a9d00824fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_model(model, test_loader, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    all_attn_weights = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, mask, y in test_loader:\n",
    "            x, mask, y = x.to(device), mask.to(device), y.to(device)\n",
    "            output, attn_weights = model(x, mask)\n",
    "            loss = criterion(output.squeeze(), y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "\n",
    "            predictions.extend(output.squeeze().cpu().numpy())\n",
    "            targets.extend(y.cpu().numpy())\n",
    "            all_attn_weights.extend(attn_weights.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader.dataset)\n",
    "    print(f\"\\nTest MSE: {avg_loss:.4f}\")\n",
    "    return predictions, targets, all_attn_weights\n",
    "\n",
    "\n",
    "def plot_loss_curves(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_attention(attn_weights, sequence, mask=None, idx=0):\n",
    "    weights = attn_weights[idx]\n",
    "    values = sequence[idx].cpu().numpy()\n",
    "    if mask is not None:\n",
    "        weights = weights * mask[idx].cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    plt.bar(range(len(weights)), weights, alpha=0.6)\n",
    "    plt.title(\"Attention Weights\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Weight\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Sequence Values:\")\n",
    "    print(values)\n",
    "    print(\"\\nAttention:\")\n",
    "    print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca0d8fe204c94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "preds, targs, _ = test_model(trained_model, test_loader)\n",
    "\n",
    "mse = mean_squared_error(targs, preds)\n",
    "mae = mean_absolute_error(targs, preds)\n",
    "\n",
    "print(f\"Test MSE: {mse:.4f} — MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df22416326a7a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, targs, attns = test_model(trained_model, test_loader)\n",
    "plot_loss_curves(train_losses, val_losses)\n",
    "visualize_attention(attns, data, masks, idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f6131fd6a67e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(preds[:100], label='Predicted')\n",
    "plt.plot(targs[:100], label='True', alpha=0.7)\n",
    "plt.title('Model Predictions vs True Values (Sample)')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e1c1b997154c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем исходные данные и маску из датасета\n",
    "test_batch = next(iter(test_loader))\n",
    "x_batch, mask_batch, y_batch = test_batch\n",
    "\n",
    "# Показываем attention и последовательность\n",
    "visualize_attention(attns, x_batch, mask_batch, idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025db64a44c3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(targs, preds, alpha=0.5)\n",
    "plt.plot([min(targs), max(targs)], [min(targs), max(targs)], 'r--', label='Ideal')\n",
    "plt.xlabel('True Value')\n",
    "plt.ylabel('Predicted Value')\n",
    "plt.title('Predicted vs True Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa92c7bf58d06731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 10  # первые 10 примеров\n",
    "x = np.arange(N)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(x - 0.2, [targs[i] for i in x], width=0.4, label='True')\n",
    "plt.bar(x + 0.2, [preds[i] for i in x], width=0.4, label='Predicted')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('True vs Predicted (first 10 samples)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12c57a8446e281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_bars(attn_weights, input_sequence, mask=None, idx=0):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    weights = attn_weights[idx]\n",
    "    values = input_sequence[idx].cpu().numpy()\n",
    "    if mask is not None:\n",
    "        weights = weights * mask[idx].cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(12, 2.5))\n",
    "    sns.barplot(x=np.arange(len(values)), y=weights, palette='coolwarm', alpha=0.6)\n",
    "    plt.xticks(ticks=np.arange(len(values)), labels=[f'{v:.2f}' for v in values], rotation=45)\n",
    "    plt.title(\"Attention weights per timestep (values shown below)\")\n",
    "    plt.xlabel(\"Sequence Element (Value)\")\n",
    "    plt.ylabel(\"Attention Weight\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_attention_bars(attns, data, masks, idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa3e48ebbc3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотреть среднее распределение attention по маске\n",
    "attn_sums = torch.stack([torch.tensor(a) for a in attns])\n",
    "attn_mean = attn_sums.mean(dim=0)\n",
    "plt.plot(attn_mean.numpy()); plt.title(\"Mean Attention Weight by Position\"); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
